\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID heredas ein
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV2017: Multi-Layer Knowledge Transfer for Neural Networks}

\author{Lennard Kiehl\\
{\tt\small lennard.kiehl@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Roman Remme\\
{\tt\small roman.remme@gmx.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The idea of transferring knowledge between different architectures of neural networks, specifically from bigger models to smaller models, has been introduced in \cite{hinton2015distilling}. Part of the motivation for this process called distilling is to create a smaller model which is faster at runtime, with the same knowledge as the bigger model. We want to extend on this idea and not only compare the last layers of both networks while training the smaller one but also add links between intermediate layers. This extension is a really canonical one as especially bigger models used for image classification have a lot of their knowledge saved in their convolutional layers which may not completely translate into the final prediction. The popular VGG-16 model introduced in \cite{DBLP:journals/corr/SimonyanZ14a} serves as the bigger model and the goal is to distill each group of convolutional layers into only one convolutional layer, for an overview of the architectures see Table \ref{tab:network_architectures}. We investigate how training hyper-parameters influence the process of successfully distilling knowledge.

\subsection{Distillation}
As it is a prerequisite for distilling to have an already trained model, to make this process worthwhile the trained model should have some kind of disadvantage at inference because distilling allows to transfer knowledge to a smaller model better suited for inference. And as shown in \cite{hinton2015distilling} distillation also works with a fraction of the original training set as well as unlabeled data because the trained model should produce reliable predictions even for unseen data.
To transfer knowledge an additional term to the loss function is introduced that links the softmax layers of both networks by calculating their cross entropy. This way the smaller model will not only have to produce the correct label but also the relationship between classes of lower probability. The softmax also has an added temperature dependency for training
\begin{equation}
	p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j/T)}
	\label{eq:softmaxtemperature}
\end{equation}
with logits $z_i$, class probabilities $p_i$ and temperature $T$, which whould normally be set to 1. Using a higher value for $T$ produces a softer probability distribution and should force the smaller model to optimize better for intermediate relationships between classes. It was shown in \cite{hinton2015distilling} that this alone serves as a very good knowledge transfer tool. We will investigate how adding a selection of other links while training will influence the distillation. For an overview of the proposed links see Table \ref{tab:network_architectures}.

\subsection{Loss}
The loss function for transferring knowledge into the small model is a weighted sum of three terms:
\begin{itemize}
	\item ``hard" loss: cross-entropy between output and correct label at temperature $T=1$.
	\item ``soft" loss: cross-entropy between output and prediction of big model at temperature $T$
	\item ``intermediate" loss: sum of MSE between linked intermediate layers
\end{itemize}
FIXME: Insert formulas here \\
The third term is the new part of our approach. An anticipated advantage is that training times should be reduced, as gradients do not have to be propagated through the whole network to reach the first layers. Typical factors in the weighted sum are
\begin{equation}
	\text{loss} = 1 \cdot \text{hard} + 10 \cdot \text{soft} + 10 \cdot \text{intermediate}.
	\label{eq:lossfactors}
\end{equation}

The main contribution comes from the ``soft" loss and the ``hard" loss while still significantly improving distillation contributes much less. This is also consistent with the ideas in \cite{hinton2015distilling}. We would place the importance of the ``intermediate" loss somewhere between these two which is relfected in Equation \ref{eq:lossfactors}.

%-------------------------------------------------------------------------
\subsection{Models}

For our experiments we use VGG-16 \cite{DBLP:journals/corr/SimonyanZ14a} as the big model. For the small model all stacks of convolutional layers have been replaced by one single convolutional layer (see Table \ref{tab:network_architectures}) and the number of fully connected layers was reduced by one. The similarity between the models is by design and makes it possible to have a maximum of 6 separate links while doing the knowledge transfer. To get the knowledge we want to transfer in the first place, the big model is trained on CIFAR10 \cite{krizhevsky2009learning} with the hyper-parameters shown in Table \ref{tab:baseline_small_big}. The convolutional layers of the big model are initialized with pre-trained weights on ImageNet \cite{ILSVRC15} while the fully connected layers are initialized randomly. The small model had to be trained from scratch as it is an uncommon architecture. The accuracies in Table \ref{tab:baseline_small_big} serve as our baseline and we expect the accuracy of the small model after distilling to be somewhere between these two test accuracies.

\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{3}{|c|}{Network configurations with linkable layers} 	\\ \hline
				&	VGG-16		&	VGG-7									\\ \hline
		\hline
				& \multicolumn{2}{c|}{input (224$\times$224 RGB image)} 	\\ \hline
		link 1	&	conv3-64	&	conv3-64								\\ 
				& 	conv3-64	&											\\ \hline
		
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 2	&	conv3-128	&	conv3-128								\\ 
				& 	conv3-128	&											\\ \hline
			
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 3	&	conv3-256	&	conv3-256								\\ 
				& 	conv3-256	&											\\ 
				& 	conv3-256	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 4	&	conv3-512	&	conv3-512								\\ 
				& 	conv3-512	&											\\ 
				& 	conv3-512	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 5	&	conv3-512	&	conv3-512								\\ 
				& 	conv3-512	&											\\ 
				& 	conv3-512	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 6	&	FC-4096		&	FC-4096									\\ 
				& 	FC-4096		&											\\ 
				& 	FC-10		&	FC-10									\\ \hline
				& \multicolumn{2}{c|}{softmax} 								\\ \hline
			
	\end{tabular}
	\end{center}
	\label{tab:network_architectures}
	\caption{\textbf{Network configurations.} The convolutional layers are denoted as conv(\textit{kernel size})-(\textit{number of channels)} and the fully 		connected layers as FC-(\textit{number of output channels}). ReLu units are omitted for brevity. The leftmost column gives the links between both networks that are added to the loss function.}
\end{table}

\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
									&	Small model & Big model \\ \hline
		Batchsize					&	40			&	FIXME	\\ \hline
		Temperature					&	2			&	FIXME	\\ \hline
		Momentum					&	0.9			&	FIXME	\\ \hline
		Weight decay				&	0.01		&	FIXME	\\ \hline
		Init learning rate (LR)		&	0.004		&	FIXME	\\ \hline
		Epochs between LR decay 	&	10			&	FIXME	\\ \hline
		Epochs						&	25			&	FIXME	\\ \hline
		Train accuracy				&	99.3\%		&	FIXME	\\ \hline
		Test accuracy				&	79.1\%		&	FIXME	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Baseline Training.} Both network architectures were trained with the given parameters to have a baseline to compare our transfer training to. The conv. layers of the big model had pre-trained weights while the small model was trained from scratch.}
	\label{tab:baseline_small_big}
\end{table}


\subsection{Dataset}
We use CIFAR-10 \cite{krizhevsky2009learning} as the dataset to train both models for all experiments. It consists of 50000 training and 10000 test RGB images of size 32$\times$32 pixels. Each image belongs to one of ten classes. To use the standard VGG architecture with these low-resolution images, they are scaled up to 224$\times$224 pixels. Each image is preprocessed by subtracting the mean RGB value, computed on the training set, from each pixel. 
 

%-------------------------------------------------------------------------
\section{Experiments}
First we perform an experiment to find out what temperatures is best suited for distillation with our choice of models (results in Table \ref{tab:LL_results}). After that we compare multiple combinations of links between intermediate layers to find out if our approach can improve the knowledge transfer over normal distilling (results in Table \ref{tab:interemediate_results}). For all experiments stochstic gradient descent with momentum as a regularizer is used as an optimizer. Furthermore after every ten epochs the learning rate is decaying by a factor of 10. This way the accuracy should stop changing significantly prior to a drop in learning rate.

\subsection{Temperature of ``soft" loss}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Temperature	&	Test accuracy	\\ \hline
		0.6			&	76.3 \%			\\ \hline
		1			&	76.5 \%			\\ \hline
		1.5			&	77.0 \%			\\ \hline
		2			&	\textbf{77.4} \%\\ \hline
		2.5			&	76.7 \%			\\ \hline
		3			&	77.2 \%			\\ \hline
		5			&	73.1 \%			\\ \hline
		10			&	64.4 \%			\\ \hline
		40			&	67.3 \%			\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Last layer transfer results.} Test accuracies after distillation using different temperatures for the softmax.}
	\label{tab:LL_results}
\end{table}

This experiment is done exactly like \cite{hinton2015distilling} describes the process of distillation. That means that the loss function consists of the ``hard" and ``soft" terms only. This is used to determine the best temperature to test our new approach. The relative weight of the "soft" loss was chosen to be ten times that of the "hard" loss. We found the best temperature $ T $ to be 2, see Table \ref{tab:LL_results}. The corresponding test set accuracy is 77.4\%, improving the small model baseline only slightly, by X\%.


\subsection{Linking intermediate layers}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Linked layers		&	$\beta$	&	Test set accuracy	\\ \hline
		1					&	10		&	78.3\%				\\ \hline
		2					&	10		&	80.3\%				\\ \hline
		3					&	10		&	83.0\%				\\ \hline
		4					&	10		&	85.6\%				\\ \hline
		5					&	10		&	\textbf{87.9}\%		\\ \hline
		%6					&	10		&	86.1\%				\\ \hline % keine gute Idee
		3, 4				&	10		&	84.8\%				\\ \hline
		2, 3, 4, 5			&	10		&	87.0\%				\\ \hline
		2, 3, 4, 5			&	40		&	\textbf{87.9}\%		\\ \hline
		2, 3, 4, 5, 6		&	10		&	78.5\%				\\ \hline
		1, 2, 3, 4, 5, 6	&	10		&	81.2\%				\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Intermediate layers transfer results.} used last layer with temperature 2 and $ \alpha = 10 $.  FIXME: Was is $\beta$? FIXME: Wo sind die .txt results aus dieser Tabelle?}
	\label{tab:interemediate_results}
\end{table}
the relative weight of the "intermediate" loss was chosen to be identical to that of the "soft" loss. When connecting multiple layers, the losses of the individual links was averaged. Table \ref{tab:interemediate_results} shows an overview of the link configurations we used, and the corresponding accuracies. First, we used one link at a time. Link 5, the last link in the convolutional part of the network, gave the best test set accuracy of 87.9\%. Using multiple intermediate layers yielded at best equally good results. But since far from all possibilities were explored, it is possible that further improvements are possible with the right choice of layers to link.

\subsection{Evolution of loss contributions}

CANT SAY ANYTHING ABOUT SOFT LOSS WITHOUT FURTHER EXPERIMENT.. 
"hard" loss drops fast on train set, but not on test set (duh..)
"intermediate" loss: fast drop in first epochs, then converges to constant value. remarkable: almost identical on train and test set --> good regularization
(would be interesting to try on very small train set..)


\section{Discussion}
This is the discussion. We are great!

\begin{itemize}
	\item good results, intermediate much better than only last
	\item choice of hyperparameters partially arbitrary: relative weights of the losses, regularization missing, choice of layers to use for transfer
	\item longer training could lead to further improvements, intermediate loss did not stop declining (though quite slow $ \rightarrow $ limit for us)
	\item decent results are achieved much faster (fewer epochs) compared to hard loss only. reason: "shorter way" to first layers
	\item potential advantages on small training sets
	\item overall: good initial results, further investigation necessary (other datasets, architectures, better tuning of hyperparameters)
\end{itemize}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
