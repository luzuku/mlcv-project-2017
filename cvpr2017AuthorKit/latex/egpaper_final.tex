\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID heredas ein
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV2017: Multi-Layer Knowledge Transfer for Neural Networks}

\author{Lennard Kiehl\\
{\tt\small lennard.kiehl@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Roman Remme\\
{\tt\small roman.remme@gmx.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The idea of transferring knowledge between different architectures of neural networks, specifically from bigger models to smaller models, has been introduced in \cite{hinton2015distilling}. Part of the motivation for this process called distilling is to create a smaller model, which is faster at runtime, with the same knowledge as the bigger model. Distilling works by introducing an additional term to the loss function that links the last layers of both networks by calculating the cross entropy between them. The softmax has an added temperature dependency
\begin{equation}
	p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j/T)}
\end{equation}
with logits $z_i$, class probabilities $p_i$ and temperature $T$, which is normally set to 1. Using a higher value for $T$ produces a softer probability distribution.
It was shown in \cite{hinton2015distilling} that this alone serves as a very good knowledge transfer tool. We want to extend on this idea and also add links between intermediate layers. The popular VGG-16 model introduced in \cite{DBLP:journals/corr/SimonyanZ14a} serves as the bigger model and the goal is to distill each group of convolutional layers into only one convolutional layer, for an overview of the architectures see Table \ref{tab:network_architectures}. We investigate how training hyper-parameters influence the process of successfully distilling knowledge.

%-------------------------------------------------------------------------
\subsection{Models}

For our experiments we use VGG-16 \cite{DBLP:journals/corr/SimonyanZ14a} as the big model. For the small model all stacks of convolutional layers have been replaced by one single convolutional layer (see Table \ref{tab:network_architectures}) and the number of fully connected layers was reduced by one. The similarity between the models is by design and makes it possible to have a maximum of 6 separate links while doing the knowledge transfer. To get the knowledge we want to transfer in the first place, the big model is trained on CIFAR10 \cite{krizhevsky2009learning} with the hyper-parameters shown in Table \ref{tab:baseline_small_big}. The convolutional layers of the big model are initialized with pre-trained weights on ImageNet \cite{ILSVRC15} while the fully connected layers are initialized randomly. The small model had to be trained from scratch as it is an uncommon architecture. The accuracies in Table \ref{tab:baseline_small_big} serve as our baseline and we expect the accuracy of the small model after distilling to be somewhere between these two values.

\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{3}{|c|}{Network Configurations with linkable layers} \\ \hline
			&	VGG-16	&	VGG-7	\\ \hline
			\hline
		& \multicolumn{2}{c|}{input (224$\times$224 RGB image)} \\ \hline
		link 1	&	conv3-64	&	conv3-64	\\ 
			& 	conv3-64	&		\\ \hline
		
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 2	&	conv3-128	&	conv3-128	\\ 
			& 	conv3-128	&		\\ \hline
			
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 3	&	conv3-256	&	conv3-256	\\ 
			& 	conv3-256	&		\\ 
			& 	conv3-256	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 4	&	conv3-512	&	conv3-512	\\ 
			& 	conv3-512	&		\\ 
			& 	conv3-512	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 5	&	conv3-512	&	conv3-512	\\ 
			& 	conv3-512	&		\\ 
			& 	conv3-512	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 6	&	FC-4096	&	FC-4096	\\ 
			& 	FC-4096	&		\\ 
			& 	FC-10	&	FC-10	\\ \hline
			& \multicolumn{2}{c|}{softmax} \\ \hline
			
	\end{tabular}
	\end{center}
	\label{tab:network_architectures}
	\caption{\textbf{Network configurations.} The convolutional layers are denoted as conv(\textit{kernel size})-(\textit{number of channels)} and the fully 		connected layers as FC-(\textit{number of output channels}). ReLu units are omitted for brevity. The leftmost column gives the links between both networks that are added to the loss function.}
\end{table}

\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
								&	small model 	&	big model	\\ \hline
		batchsize					&	40			&	40		\\ \hline
		momentum				&	0.9			&	0.9		\\ \hline
		weight decay				&	0.00002		&	0.00002	\\ \hline
		init learning rate (LR)			&	0.004		&	0.004	\\ \hline
		epochs between LR decay 	&	10			&	10		\\ \hline
		epochs					&	25			&	25		\\ \hline
		train accuracy				&	84.8\%		&	84.8\%	\\ \hline
		test accuracy				&	84.8\%		&	84.8\%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Baseline Training.} Both network architectures were trained with the given parameters to have a baseline to compare our transfer 	training to. The conv. layers of the big model had pre-trained weights while the small model was trained from scratch.}
	\label{tab:baseline_small_big}
\end{table}

\subsection{Loss}
The loss function for transferring knowledge into the small model is a weighted sum of three terms:
\begin{itemize}
	\item ``hard" loss: cross-entropy between output and correct label at temperature $T=1$.
	\item ``soft" loss: cross-entropy between output and prediction of big model at temperature $T$
	\item ``intermediate" loss: sum of MSE between linked intermediate layers
\end{itemize}
INSERT FORMULAS HERE \\
The third term is the new part of our approach. A theoretical advantage is that training times should be reduced, as gradients do not have to be propagated through the whole network to reach the first layers. Typical factors in the weighted sum are
\begin{equation}
	\text{loss} = 0.05 \cdot \text{hard} + 0.6 \cdot \text{soft} + 0.35 \cdot \text{intermediate}.
\end{equation}



\subsection{Dataset}
We use CIFAR-10 \cite{krizhevsky2009learning} as the dataset to train both models on for all experiments. It consists of 50000 training and 10000 test RGB images of size 32$\times$32 pixels. Each image belongs to one of ten classes. To use the standard VGG architecture with these low-resolution images, they are scaled up to 224$\times$224 pixels. Each image is preprocessed by subtracting the mean RGB value, computed on the training set, from each pixel. 
 

%-------------------------------------------------------------------------
\section{Experiments}
We used stochastic gradient descent with momentum 0.9 (CITATION NEEDED?) as an optimizer. We started with a learning rate of 0.004 and let it decay by a factor of 10 every 10 epochs for 25 epochs. This way the accuracy stopped changing significantly prior to the drops in learning rate.

\subsection{Temperature of soft loss}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Temperature	&	Test set accuracy\\ \hline
		0.6	&	76.3 \%	\\ \hline
		1	&	76.5 \%	\\ \hline
		1.5	&	77.0 \%	\\ \hline
		2	&	\textbf{77.4} \%	\\ \hline
		2.5	&	76.7 \%	\\ \hline
		3	&	77.2 \%	\\ \hline
		5	&	73.1 \%	\\ \hline
		10	&	64.4 \%	\\ \hline
		40	&	67.3 \%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Last layer transfer results.} bla bla}
	\label{tab:LL_results}
\end{table}

Before adding the "intermediate" loss, we trained the model only using the transfer method from \cite{hinton2015distilling}. The relative weigth of the "soft" loss was chosen to be ten times that of the "hard" loss. We found the best temperature $ T $ to be approximately 2, see Table \ref{tab:LL_results}. This value was also used for all following experiments. The corresponding test set accuracy is 77.4\%, improving the small model baseline only slightly, by X\%.


\subsection{Linking intermediate layers}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Linked layers	&	$\beta$	&	Test set accuracy\\ \hline
		1	&	10	&	78.3\%	\\ \hline
		2	&	10	&	80.3\%	\\ \hline
		3	&	10	&	83.0\%	\\ \hline
		4	&	10	&	85.6\%	\\ \hline
		5	&	10	&	\textbf{87.9}\%	\\ \hline
		6	&	10	&	86.1\%	\\ \hline
		3, 4	&	10	&	84.8\%	\\ \hline
		2, 3, 4, 5	&	10	&	87.0\%	\\ \hline
		2, 3, 4, 5	&	40	&	\textbf{87.9}\%	\\ \hline
		2, 3, 4, 5, 6	&	10	&	78.5\%	\\ \hline
		1, 2, 3, 4, 5, 6	&	10	&	81.2\%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Intermediate Layers transfer results.} used last layer with temperature 2 and $ \alpha = 10 $. MEINE AUSWAHL SIEHT HIER ZIEMLICH DUMM AUS.. VIELLEICHT WAS WEGLASSEN}
	\label{tab:interemediate_results}
\end{table}
the relative weight of the "intermediate" loss was chosen to be identical to that of the "soft" loss. When connecting multiple layers, the losses of the individual links was averaged. Table \ref{tab:interemediate_results} shows an overview of the link configurations we used, and the corresponding accuracies. First, we used one link at a time. Link 5, the last link in the convolutional part of the network, gave the best test set accuracy of 87.9\%. Using multiple intermediate layers yielded at best equally good results. But since far from all possibilities were explored, it is possible that further improvements are possible with the right choice of layers to link.

\subsection{Evolution of loss contributions}

CANT SAY ANYTHING ABOUT SOFT LOSS WITHOUT FURTHER EXPERIMENT.. 
"hard" loss drops fast on train set, but not on test set (duh..)
"intermediate" loss: fast drop in first epochs, then converges to constant value. remarkable: almost identical on train and test set --> good regularization
(would be interesting to try on very small train set..)


\section{Discussion}
This is the discussion. We are great!

\begin{itemize}
	\item good results, intermediate much better than only last
	\item choice of hyperparameters partially arbitrary: relative weights of the losses, regularization missing, choice of layers to use for transfer
	\item longer training could lead to further improvements, intermediate loss did not stop declining (though quite slow $ \rightarrow $ limit for us)
	\item decent results are achieved much faster (fewer epochs) compared to hard loss only. reason: "shorter way" to first layers
	\item potential advantages on small training sets
	\item overall: good initial results, further investigation necessary (other datasets, architectures, better tuning of hyperparameters)
\end{itemize}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
