\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID heredas ein
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV2017: Multi-Layer Knowledge Transfer for Neural Networks}

\author{Lennard Kiehl\\
{\tt\small lennard.kiehl@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Roman Remme\\
{\tt\small roman.remme@gmx.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The idea of transferring knowledge between different architectures of neural networks, specifically from bigger models to smaller models, has been introduced in \cite{hinton2015distilling}. Part of the motivation for this process called dis<tilling is to create a smaller model which is faster at runtime, with the same knowledge as the bigger model. We want to extend on this idea and not only compare the last layers of both networks while training the smaller one but also add links between intermediate layers. This extension is a really canonical one as especially bigger models used for image classification have a lot of their knowledge saved in their convolutional layers which may not completely translate into the final prediction. The popular VGG-16 model introduced in \cite{DBLP:journals/corr/SimonyanZ14a} serves as the bigger model and the goal is to distill each group of convolutional layers into only one convolutional layer, for an overview of the architectures see Table \ref{tab:network_architectures}. We investigate how training hyper-parameters influence the process of successfully distilling knowledge.

\subsection{Distillation}
As it is a prerequisite for distilling to have an already trained model, to make this process worthwhile the trained model should have some kind of disadvantage at inference because distilling allows to transfer knowledge to a smaller model better suited for inference. And as shown in \cite{hinton2015distilling} distillation also works with a fraction of the original training set as well as unlabeled data because the trained model should produce reliable predictions even for unseen data.
To transfer knowledge an additional term to the loss function is introduced that links the softmax layers of both networks by calculating their cross entropy. This way the smaller model will not only have to produce the correct label but also the relationship between classes of lower probability. The softmax also has an added temperature dependency for training
\begin{equation}
	q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j/T)}
	\label{eq:softmaxtemperature}
\end{equation}
with logits $z_i$, predicted class probabilities $q_i$ and temperature $T$, which whould normally be set to 1. Using a higher value for $T$ produces a softer probability distribution and should force the smaller model to optimize better for intermediate relationships between classes. It was shown in \cite{hinton2015distilling} that this alone serves as a very good knowledge transfer tool. We will investigate how adding a selection of other links while training will influence the distillation. For an overview of the proposed links see Table \ref{tab:network_architectures}.

\subsection{Loss}
The loss function for transferring knowledge into the small model is a weighted sum of three terms:

\begin{itemize}
	\item ``hard" loss: cross-entropy between output and correct label at temperature $T=1$
		\begin{equation}
			\text{hard} = -\sum_i \text{label}_i(x) \log \text{small}_i(x)
			\label{eq:hardloss}
		\end{equation}
	\item ``soft" loss: cross-entropy between output and prediction of big model at temperature $T$
		\begin{equation}
			\text{soft} = -\sum_i \text{big}_i(x) \log \text{small}_i(x)
			\label{eq:softloss}
		\end{equation}
	\item ``intermediate" loss: sum of MSE between linked intermediate layers
		\begin{equation}
			\text{intermediate} = \frac{1}{N} \sum_i^N \text{MSE}(\text{link}_i)
			\label{eq:intermediateloss}
		\end{equation}
\end{itemize}

The third term is the new part of our approach. The ``intermediate" loss averages the MSE over all links that were selected. With this we make sure that the magnitude of the loss stays consistent between experiments with different numbers of links. An anticipated advantage is that training times should be reduced, as gradients do not have to be propagated through the whole network to reach the first layers. Typical factors in the weighted sum are

\begin{equation}
	\text{total loss} = \text{hard} + 10 \cdot \text{soft} + 10 \cdot \text{intermediate}
	\label{eq:totalloss}
\end{equation}

The main contributions come equally from ``soft"  and ``intermediate" loss. While still significantly improving distillation the ``hard" loss contribution is much less. This is also consistent with the ideas in \cite{hinton2015distilling}. We would place the importance of the ``intermediate" loss somewhere between these two which is relfected in Equation \ref{eq:lossfactors}.

%-------------------------------------------------------------------------
\subsection{Models}

For our experiments we use VGG-16 \cite{DBLP:journals/corr/SimonyanZ14a} as the big model. For the small model all stacks of convolutional layers have been replaced by one single convolutional layer (see Table \ref{tab:network_architectures}) and the number of fully connected layers was reduced by one. The similarity between the models is by design and makes it possible to have a maximum of 6 separate links while doing the knowledge transfer. To get the knowledge we want to transfer in the first place, the big model is trained on CIFAR10 \cite{krizhevsky2009learning} with the hyper-parameters shown in Table \ref{tab:baseline_small_big}. The convolutional layers of the big model are initialized with pre-trained weights on ImageNet \cite{ILSVRC15} while the fully connected layers are initialized randomly. The small model had to be trained from scratch as it is an uncommon architecture. The accuracies in Table \ref{tab:baseline_small_big} serve as our baseline and we expect the accuracy of the small model after distilling to be somewhere between these two test accuracies.

\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{3}{|c|}{Network configurations with linkable layers} 	\\ \hline
				&	VGG-16		&	VGG-7									\\ \hline
		\hline
				& \multicolumn{2}{c|}{input (224$\times$224 RGB image)} 	\\ \hline
		link 1	&	conv3-64	&	conv3-64								\\ 
				& 	conv3-64	&											\\ \hline
		
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 2	&	conv3-128	&	conv3-128								\\ 
				& 	conv3-128	&											\\ \hline
			
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 3	&	conv3-256	&	conv3-256								\\ 
				& 	conv3-256	&											\\ 
				& 	conv3-256	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 4	&	conv3-512	&	conv3-512								\\ 
				& 	conv3-512	&											\\ 
				& 	conv3-512	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 5	&	conv3-512	&	conv3-512								\\ 
				& 	conv3-512	&											\\ 
				& 	conv3-512	&											\\ \hline
				& \multicolumn{2}{c|}{maxpool 2$\times$2} 					\\ \hline
		link 6	&	FC-4096		&	FC-4096									\\ 
				& 	FC-4096		&											\\ 
				& 	FC-10		&	FC-10									\\ \hline
				& \multicolumn{2}{c|}{softmax} 								\\ \hline
			
	\end{tabular}
	\end{center}
	\label{tab:network_architectures}
	\caption{\textbf{Network configurations.} The convolutional layers are denoted as conv(\textit{kernel size})-(\textit{number of channels)} and the fully 		connected layers as FC-(\textit{number of output channels}). ReLu units are omitted for brevity. The leftmost column gives the links between both networks that are added to the loss function.}
\end{table}

\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
									&	Small model & Big model \\ \hline
		Batchsize					&	40			&	40		\\ \hline
		Momentum					&	0.9			&	.9		\\ \hline
		Weight decay				&	0.01		&	0.0002	\\ \hline
		Init learning rate (LR)		&	0.004		&	0.004	\\ \hline
		Epochs between LR decay 	&	10			&	25		\\ \hline
		Epochs						&	25			&	100		\\ \hline
		Train accuracy				&	99.3\%		&	100.0\%	\\ \hline
		Test accuracy				&	79.1\%		&	91.4\%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Baseline Training.} Both network architectures were trained with the given parameters to have a baseline to compare our transfer training to. The conv. layers of the big model had pre-trained weights while the small model was trained from scratch.}
	\label{tab:baseline_small_big}
\end{table}


\subsection{Dataset}
We use CIFAR-10 \cite{krizhevsky2009learning} as the dataset to train both models for all experiments. It consists of 50000 training and 10000 test RGB images of size 32$\times$32 pixels. Each image belongs to one of ten classes. To use the standard VGG architecture with these low-resolution images, they are scaled up to 224$\times$224 pixels. Each image is preprocessed by subtracting the mean RGB value, computed on the training set, from each pixel. 
 

%-------------------------------------------------------------------------
\section{Experiments}
First we perform an experiment to find out what temperatures is best suited for distillation with our choice of models (results in Table \ref{tab:LL_results}). Next we compare multiple combinations of links between intermediate layers to find out if our approach can improve the knowledge transfer over normal distilling (results in Table \ref{tab:interemediate_results}). For all experiments stochstic gradient descent with momentum as a regularizer is used as an optimizer. Furthermore after every ten epochs the learning rate is decaying by a factor of 10. This way the accuracy should stop changing significantly prior to a drop in learning rate.

\subsection{Temperature of ``soft" loss}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Temperature	&	Test accuracy	\\ \hline
		0.6			&	76.3 \%			\\ \hline
		1			&	76.5 \%			\\ \hline
		1.5			&	77.0 \%			\\ \hline
		2			&	\textbf{77.4} \%\\ \hline
		2.5			&	76.7 \%			\\ \hline
		3			&	77.2 \%			\\ \hline
		5			&	73.1 \%			\\ \hline
		10			&	64.4 \%			\\ \hline
		40			&	67.3 \%			\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Distillation using only ``hard" and ``soft" loss.} Test accuracies after distillation using different temperatures for the softmax.}
	\label{tab:LL_results}
\end{table}

This experiment is done exactly like \cite{hinton2015distilling} describes the process of distillation. That means that the loss function consists of the ``hard" and ``soft" terms only. This is used to determine the best temperature to test our new approach. The relative weight of the "soft" loss was chosen to be ten times that of the "hard" loss. We found the best temperature $ T $ to be 2, see Table \ref{tab:LL_results}. The corresponding test accuracy is only 77.4\%, which is less than our baseline of 79.1\% for the small model. This is due to a lack of further experiments. Optimizing these numbers takes a lot of time and especially compute time. We think that a missing regularization is the main cause and that normally this accuracy should be a little higher than our baseline. This would also be consistent with the findings in \cite{hinton2015distilling}. It should be noted that the goal of determining the best temperature has still been achieved and we can proceed with the main experiment.

\subsection{Linking intermediate layers}

\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Linked layers		&	Test accuracy		\\ \hline
		1					&	78.3\%				\\ \hline
		2					&	80.3\%				\\ \hline
		3					&	83.0\%				\\ \hline
		4					&	85.6\%				\\ \hline
		5					&	\textbf{87.9}\%		\\ \hline
		6					&	86.1\%				\\ \hline % keine gute Idee % doch, anders als soft loss
		3, 4				&	84.8\%				\\ \hline
		2, 3, 4, 5			&	87.0\%				\\ \hline
		2, 3, 4, 5			&	\textbf{87.9}\%		\\ \hline
		2, 3, 4, 5, 6		&	78.5\%				\\ \hline
		1, 2, 3, 4, 5, 6	&	81.2\%				\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Distillation with added ``intermediate" loss.} Test accuracies for different sets of links between intermediate layers ($T=2$).}
	\label{tab:interemediate_results}
\end{table}
This is the main experiment of this paper. We want to investigate if adding links between intermediate layers while distilling knowledge between models can improve the transfer and have an impact on test accuracies. If this is the case we also want to compare different sets of intermediate links. The results can be seen in Table \ref{tab:interemediate_results}. The relative weight of the "intermediate" loss was chosen to be identical to that of the "soft" loss as we think it should be just as important for the transfer. However to make results with different numbers of links comparable the ``intermediate" loss is always an average over all losses that result from links between intermediate layers. If we would not have implemented some kind of mechanism to guarantee a more or less consistent loss, it could have been the case that more links would lead to a difference in magnitude of the ``intermediate" loss.\\
First, we use one link at a time. Link 5, the last link in the convolutional part of the network, has the best test accuracy of 87.9\%. This is a 13\% improvement over our baseline of 79.1\%. Using multiple links between intermediate layers yielded at best equally good results. But since far from all possibilities were explored, it is possible that further improvements could be achieved with the right choice of layers to link.

%\subsection{Evolution of loss contributions}

%CANT SAY ANYTHING ABOUT SOFT LOSS WITHOUT FURTHER EXPERIMENT.. 
%"hard" loss drops fast on train set, but not on test set (duh..)
%"intermediate" loss: fast drop in first epochs, then converges to constant value. remarkable: almost identical on train and test set --> good regularization
%(would be interesting to try on very small train set..)


\section{Discussion}
This paper investigated an extension of the process called distillation originally proposed by Hinton et al.\cite{hinton2015distilling}. Distillation is a method to transfer knowledge from an already trained model to a new one that requires less time and data than training from scratch. Ideally the new model is similar in architecture but with fewer layers to make it faster at runtime than the already available trained model. Distilling knowledge works by linking the softmax layers of both networks by calculating their cross-entropy, making the ``knowledge" of relationships in between classes an additional target to the normal onehot encoded label. We extended this idea by adding links between different intermediate layers but using the MSE instead, because this compares activations and not probability distributions.\\
For our experiments we used the popular VGG-16 and a slimmed-down version with each stack of layers being reduced to one, effectively being ``VGG-7" (see Table \ref{tab:network_architectures}). From our comparison of different temperatures $T$ for calculating the softmax (Equation \ref{eq:softmaxtemperature}) in Table \ref{tab:LL_results}, we decided to use $T=2$ for comparing the effectiveness between different sets of intermediate links. The resulting test accuracies can be seen in Table \ref{tab:interemediate_results}. From this we conclude that adding links between intermediate layers while doing knowledge distillation improves the transfer significantly. Not only is the training time reduced by the fact that it doesn't take multiple epochs for the gradient to reach layers in the front of the model, but the test accuracies on CIFAR-10 \cite{krizhevsky2009learning} improved by 13\% to 87.9\% over our baseline of 79.1\% when training from scratch. While doing these experiments we observed a partial invariance to the choice of hyperparameters for training, leading us to believe that additional links might also make the distillation more robust.\\
Given the scope and our investment of time in this project, we think that there is still room to improve and investigate. Although our results look promising we propose further experiments with other architectures and datasets. It might also be possible that our tuning of the hyperparameters is off and the observed invariance is not real. With this is in mind, we think that knowledge distillation is an idea worth investigating further, especially considering the discrepancy between the ever growing size of top-performing models and the spread of machine learning into small handheld devices. If at least a few future AI services would aim to be independent of a fast internet connection, knowledge distillation into feasible models might be a necessity.
%\begin{itemize}
%	\item good results, intermediate much better than only last
%	\item choice of hyperparameters partially arbitrary: relative weights of the losses, regularization missing, choice of layers to use for transfer
%	\item longer training could lead to further improvements, intermediate loss did not stop declining (though quite slow $ \rightarrow $ limit for us)
%	\item decent results are achieved much faster (fewer epochs) compared to hard loss only. reason: "shorter way" to first layers
%	\item potential advantages on small training sets
%	\item overall: good initial results, further investigation necessary (other datasets, architectures, better tuning of hyperparameters)
%\end{itemize}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
