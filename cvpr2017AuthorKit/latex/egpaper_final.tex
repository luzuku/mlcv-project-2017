\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID heredas ein
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{MLCV2017: Multi-Layer Knowledge Transfer for Neural Networks}

\author{Lennard Kiehl\\
{\tt\small lennard.kiehl@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Roman Remme\\
{\tt\small roman.remme@gmx.de}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully-justified italicized text, at the top
   of the left-hand column, below the author and affiliation
   information. Use the word ``Abstract'' as the title, in 12-point
   Times, boldface type, centered relative to the column, initially
   capitalized. The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The idea of transferring knowledge between different architectures of neural networks, specifically from bigger models to smaller models, has been introduced in \cite{hinton2015distilling}. Part of the motivation for this process called distilling is to create a smaller model, which is faster at runtime, with the same knowledge as the bigger model. Distilling works by introducing an additional term to the loss function that links the last layers of both networks by calculating the cross entropy between them. The softmax has an added temperature dependency
\begin{equation}
	p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j/T)}
\end{equation}
with logits $z_i$, class probabilities $p_i$ and temperature $T$, which is normally set to 1. Using a higher value for $T$ produces a softer probability distribution.
It was shown in \cite{hinton2015distilling} that this alone serves as a very good knowledge transfer tool. We want to extend on this idea and also add links between intermediate layers. The popular VGG-16 model introduced in \cite{DBLP:journals/corr/SimonyanZ14a} serves as the bigger model and the goal is to distill each group of convolutional layers into only one convolutional layer, for an overview of the architectures see Table \ref{tab:network_architectures}. We investigate how training hyper-parameters influence the process of successfully distilling knowledge.

%-------------------------------------------------------------------------
\subsection{Models}

For our experiments we use VGG-16 \cite{DBLP:journals/corr/SimonyanZ14a} as the big model. For the small model all stacks of convolutional layers have been replaced by one single convolutional layer (see Table \ref{tab:network_architectures}) and the number of fully connected layers was reduced by one. The similarity between the models is by design and makes it possible to have a maximum of 6 separate links while doing the knowledge transfer. To get the knowledge we want to transfer in the first place, the big model is trained on CIFAR10 \cite{krizhevsky2009learning} with the hyper-parameters shown in Table \ref{tab:baseline_small_big}. The convolutional layers of the big model are initialized with pre-trained weights on ImageNet \cite{ILSVRC15} while the fully connected layers are initialized randomly. The small model had to be trained from scratch as it is an uncommon architecture. The accuracies in Table \ref{tab:baseline_small_big} serve as our baseline and we expect the accuracy of the small model after distilling to be somewhere between these two values.

\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\multicolumn{3}{|c|}{Network Configurations with linkable layers} \\ \hline
			&	VGG-16	&	VGG-7	\\ \hline
			\hline
		& \multicolumn{2}{c|}{input (224$\times$224 RGB image)} \\ \hline
		link 1	&	conv3-64	&	conv3-64	\\ 
			& 	conv3-64	&		\\ \hline
		
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 2	&	conv3-128	&	conv3-128	\\ 
			& 	conv3-128	&		\\ \hline
			
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 3	&	conv3-256	&	conv3-256	\\ 
			& 	conv3-256	&		\\ 
			& 	conv3-256	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 4	&	conv3-512	&	conv3-512	\\ 
			& 	conv3-512	&		\\ 
			& 	conv3-512	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 5	&	conv3-512	&	conv3-512	\\ 
			& 	conv3-512	&		\\ 
			& 	conv3-512	&		\\ \hline
			& \multicolumn{2}{c|}{maxpool 2$\times$2} \\ \hline
		link 6	&	FC-4096	&	FC-4096	\\ 
			& 	FC-4096	&		\\ 
			& 	FC-10	&	FC-10	\\ \hline
			& \multicolumn{2}{c|}{softmax} \\ \hline
			
	\end{tabular}
	\end{center}
	\label{tab:network_architectures}
	\caption{\textbf{Network configurations.} The convolutional layers are denoted as conv(\textit{kernel size})-(\textit{number of channels)} and the fully 		connected layers as FC-(\textit{number of output channels}). ReLu units are omitted for brevity. The leftmost column gives the links between both networks that are added to the loss function.}
\end{table}

\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
								&	small model 	&	big model	\\ \hline
		batchsize					&	40			&	40		\\ \hline
		momentum				&	0.9			&	0.9		\\ \hline
		weight decay				&	0.00002		&	0.00002	\\ \hline
		init learning rate (LR)			&	0.004		&	0.004	\\ \hline
		epochs between LR decay 	&	10			&	10		\\ \hline
		epochs					&	25			&	25		\\ \hline
		train accuracy				&	84.8\%		&	84.8\%	\\ \hline
		test accuracy				&	84.8\%		&	84.8\%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Baseline Training.} Both network architectures were trained with the given parameters to have a baseline to compare our transfer 	training to. The conv. layers of the big model had pre-trained weights while the small model was trained from scratch.}
	\label{tab:baseline_small_big}
\end{table}

\subsection{Loss}
The loss function for transferring knowledge into the small model is a weighted sum of three terms:
\begin{itemize}
	\item ``hard" loss: cross-entropy between output and correct label at temperature $T=1$.
	\item ``soft" loss: cross-entropy between output and prediction of big model at temperature $T$
	\item ``intermediate" loss: sum of MSE between linked intermediate layers
\end{itemize}
The third term is the new part of our approach. A theoretical advantage is that training times should be reduced, as gradients do not have to be propagated through the whole network to reach the first layers. Typical factors in the weighted sum are
\begin{equation}
	\text{loss} = 0.05 \cdot \text{hard} + 0.6 \cdot \text{soft} + 0.35 \cdot \text{intermediate}.
\end{equation}



\subsection{Dataset}
We use CIFAR-10 \cite{krizhevsky2009learning} as the dataset to train both models on for all experiments. It consists of 50000 training and 10000 test RGB images of size 32$\times$32 pixels. Each image belongs to one of ten classes. To use the standard VGG architecture with these low-resolution images, they are scaled up to 224$\times$224 pixels. Each image is preprocessed by subtracting the mean RGB value, computed on the training set, from each pixel. 
 

%-------------------------------------------------------------------------
\section{Experiments}
We used stochastic gradient descent with momentum 0.9 (CITATION NEEDED?) as an optimizer. We started with a learning rate of 0.004 and let it decay by a factor of 10 every 10 epochs for 25 epochs. 

\subsection{Temperature of soft loss}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Temperature	&	Test set accuracy\\ \hline
		0.6	&	76.3 \%	\\ \hline
		1	&	76.5 \%	\\ \hline
		1.5	&	77.0 \%	\\ \hline
		2	&	\textbf{77.4} \%	\\ \hline
		2.5	&	76.7 \%	\\ \hline
		3	&	77.2 \%	\\ \hline
		5	&	73.1 \%	\\ \hline
		10	&	64.4 \%	\\ \hline
		40	&	67.3 \%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Last layer transfer results.} bla bla}
	\label{tab:LL_results}
\end{table}

\subsection{Linking intermediate layers}
\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Linked layers	&	$\beta$	&	Test set accuracy\\ \hline
		3	&	10	&	85.9\%	\\ \hline
		2, 3, 4, 5	&	10	&	87.0\%	\\ \hline
		2, 3, 4, 5	&	40	&	87.9\%	\\ \hline
		5	&	10	&	87.7\%	\\ \hline
		3, 4	&	10	&	84.8\%	\\ \hline
		1, 2, 3, 4, 5, 6	&	10	&	81.2\%	\\ \hline
		2, 3, 4, 5, 6	&	10	&	78.5\%	\\ \hline
	\end{tabular}
	\end{center}
	\caption{\textbf{Intermediate Layers transfer results.} used last layer with temperature 2 and $ \alpha = 10 $}
	\label{tab:interemediate_results}
\end{table}

\subsection{Evolution of loss contributions}

\section{Discussion}
This is the discussion. We are great!


{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
